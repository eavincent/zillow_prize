---
title: "Machine Learning in R"
author: "Liz Vincent"
date: "9/4/2017"
output: html_document
---

##Supervised learning

* Train a machine to learn from prior examples
* Classification: concept to be learned is a set of categories

###Classification tasks - K Nearest neighbors
* Coordinates in a feature-space - e.g. red signs close to other red signs
* R function `knn(train, test, cl)` (k nearest neighbors) in `class` package equires training data, test data,
  and classification labels for training data
* k determines the size of the neighborhood
* if k=1, only the single nearest neighbor is used to classify the test data
* if k>1, the majority category for the neighbors is chosen
* In the case of a tie, winner typically chosen at random
* Larger k can be better if the nearest neighbor(s) happen to be a different classification, but bigger is not always better
* Smaller neighborhoods mean the classifier can detect subtler patterns
* Optimal k depends on complexity of the pattern to be learned as well as the influence of noisy data - 
  larger k filters out noise better but may ignore subtle structures of the data
* Rule of thumb: set `k = sqrt(# obs in training group)`
* May be useful to determine the certainty of a prediction - can set `prob=T` in the `knn()` function
  and inspect the probabilities using attr(x, "prob")
* `aggregate(x~y,data,function)`
* Confusion matrix using `table(predicted,actual)`
* Computes overall accuracy using `mean(predicted == actual)`
* `knn()` and most nearest neighbors functions assume numeric data bc distance between categorical variables cannot be calculated
* In instances where a categorical variable cannot be converted to numberic, a dummy binary variable is created for each possibility except one
  (e.g. if the options are rectangle, diamond, and octagon there would be 2 dummy variables rectangle and diamond. Rectangles are given 1 0, 
  diamonds are 0 1, and octagons are 0 0.)
* Dummy-coded variables can be used directly in distance functions
* For all categories to contribute equally, rescale categories such that they are all on the same scale (e.g. a 0 to 1 scale)
    
###Bayesian methods
* Joint probabilities (P(A and B)) and independent events
* Conditional probabilities (P(A | B)) and dependent events
* P(A | B) = P(A and B) / P(B)
* Naive Bayes: estiamte conditional probability of an outcome
* `naivebayes` package function `naive_bayes(prediction ~ predictor, data)` builds a model
* `predict(m, future_conditions)` predicts outcomes based on future predictions using the Naive Bayes model `m`.
  Use `type="prob"` argument to get probabilities for each outcome.
* More conditionals make the computation more 
* The naive assumption is that the events are independent
* Joint probability can be calculated by multiplying individual probabilities - 
  can calculate multiple overlaps in events by multiplying simpler overlaps
* Laplace correction: add a constant (usually 1) to each probability event so that things that have not happened yet 
  but still _could_ happen in the future (i.e. infrequent events) are not always predicted as 0 due to the fact that they haven't happened _yet_
* Numeric properties and uncategorized text are difficult for naive bayes - use bins to get around this problem